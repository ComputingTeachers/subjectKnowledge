Rational for Subject Audit Rework
=================================

I deliver the PGCE in Secondary Computing course. I provide a framework for trainee teachers to perform a gap analysis of their subject knowledge and track their subject knowledge development throughout the course. Trainee teachers subject knowledge is not assessed in isolation and is instead assessed by proxy of evidence to meet the UK Teacher Standards. 

Students are required to self assess their skills and identify their subject knowledge gaps (Rhoades 2014). Rhoades states "any overestimation or even underestimation of your knowledge will be counter productive". Trainee's self assessment of subject knowledge is varied and inaccurate as trainees have no benchmark or reference point to based their skills assessment upon (Schmid 2021 1.1). Trainees lack the knowledge of the curriculum to judge if their stills are sufficient. (Jones et al., 2019) cited McNamara (1991) in stating; a teachers' restricted subject knowledge affects the quality of teaching and causes serious deficiencies in student learning, this deficiency is not immediately identifiable. (D2 V pedagogical research)

Currently trainees are asked to rate their knowledge category's on a scale of 1 (insecure knowledge) to 4 (I could teach this) each school term.
Some examples of the current categories:
* Understand why computer hardware needs computer software (programs)
* Know what computer programming syntax is
* Explain what a computer is

The categories above are vague; How could a trainee identify/justify that they have moved between a level 2 to level 3? Without the context of the level of curriculum that is being taught (e.g. key-stage 2, 3, 4 or 5) and the curriculum content knowledge, the trainees might answers could vary significantly.

> Existing questionnaires have been criticized regarding the fuzzy, technology-unspecific, and content-agnostic wording of questionnaire items, which ask participants to rate the “appropriateness” of their competencies
(Schmid, Brianza and Petko, 2021)

When assessing my trainees I have asked "can you justify/evidence the growth in your subject knowledge". It has been difficult for them to provide evidence of their growth. (A3 assessment give feedback)

I propose two changes to enhance the effectiveness of the subject knowledge audit. We could 1. tighten the scope of the categories and 2. clarify the level criteria. This would allow for more accurate self assessment and in turn enhance their effectiveness as teachers.

My re-worked subject knowledge audit proposes a new level system:
1. Theoretical understanding (evidence - maybe past exam question or previous degree module grade)
2. Have a practical implementation on a computer (evidence - link to github? screenshot?)
3. Have taught (evidence resources created or lesson plans)

It is clear from the evidence they can present which level they are operating at.
The categories will all be taken from the Keystage 5 curriculum. The earlier keystage's are a subset of this knowledge.
The categories will be correlated with industry practice. e.g.
* Algorithms: Sorting: bubble, quick, merge, selection, insertion (2 of)
* Use of remote pair-programming live coding tool
* Use of unit-tests


This new system is much clearer and precise. The ambiguity has been removed and the categories relate directly to practical skills required in the classroom. This impact of this alteration will be assessed in more details as part of the SSPI module next term. (D2 V pedagogical research)

My proposed subject knowledge audit is only designed to address 'Technological Knowledge' and not the other strands of 'Content Knowledge' and 'Pedagogical Knowledge' from the prominent model of teacher expertise Technological Pedagogical Content Knowledge (TPACK) (Koehler and Mishra, 2014) (D2 V pedagogical research)

Ultimately assessing subject knowledge for trainee teachers is a secondary concern. What matters is that they can deliver good lessons (V4 wider context). Subject knowledge could ultimately be assessed through the lesson content that the trainees create and deliver. This approach is confirmed by (Blömeke and Delaney, 2014) who state that we should be moving beyond self-reported subject knowledge audits. (K1 subject material)


### Reference list

Blömeke, S. and Delaney, S. (2014). Assessment of Teacher Knowledge Across Countries: A Review of the State of Research. International Perspectives on Teacher Knowledge, Beliefs and Opportunities to Learn, pp.541–585.

Connell, A., Edwards, A.D. and Hramiak, A. (2015). A practical guide to teaching computing and ICT in the secondary school. Abingdon, Oxon ; New York, Ny: Routledge. Chapter 1: Developing your capability to teach Computing. Gavin Rhoades pg 10.

Jones, L.C.R., McDermott, H.J., Tyrer, J.R. and Zanker, N.P. (2019). The effect of teacher’s confidence on technology and engineering curriculum provision. International Journal of Technology and Design Education.

Koehler, M.J. and Mishra, P. (2014). Handbook of Technological Pedagogical Content Knowledge (TPCK) for Educators. [online] : Routledge. Available at: https://doi.org/10.4324/9781315759630 [Accessed 10 Aug. 2021].
‌
Pu, S., Ahmad, N.A., Khambari, M.N.M. and Yap, N.K. (2020). Factors Affecting Practical Knowledge Acquisi-tion of Pre-service Computer Science Teachers During the Practicum: A Multiple Regression Analysis. International Journal of Learning, Teaching and Educational Research, 19(2), pp.214–230.

Schmid, M., Brianza, E. and Petko, D. (2021). Self-reported technological pedagogical content knowledge (TPACK) of pre-service teachers in relation to digital technology use in lesson plans. Computers in Human Behavior, 115, p.106586.

Thorsnes, J., Rouhani, M. and Divitini, M. (2020). In-Service Teacher Training and Self-efficacy. Informatics in Schools. Engaging Learners in Computational Thinking, pp.158–169.
