# Developing an effective tool to enable trainee teachers to perform an accurate gap analysis of their existing skill-set

Abstract
--------

As a Teacher Educator for Secondary School Computing, I want to improve trainee teachers pedagogical development. 
Trainee teachers are required to show evidence of the their subject knowledge development to meet the UK teachers' standards when undertaking their 1 year PGCE course. 
My intervention was to improve the tooling that trainee teachers use to self evaluate their subject knowledge.
My experimental audit was not effective. It was not aligned with school curriculums or existing skill sets. This caused stress for my trainees and alienated my mentors.
The conclusions drawn from this trial are: to replace the subject knowledge audit with existing national training initiatives (which provide trainees with bursaries as an incentive).


The Context/Problem/Question
----------------------------

I have a background in secondary education (7 years), commercial software engineering (10 years), higher education lecturing (including computing BSc) (4 years). My skill set gives me experience of teaching computing to 11 year old's though to professional developers. Given my history; I hope my insight and perspective can influence formal education in a positive and meaningful way.


### Enhancing Teaching, Assessing and Supporting HE (ETAS) - Question

I wrote a short piece earlier this year raising question the issues about supporting teachers subject knowledge development in computing. A few important excerpts are listed below:

Students are required to self assess their skills and identify their subject knowledge gaps (Rhoades 2014). Rhoades states "any overestimation or even underestimation of your knowledge will be counter productive". Trainee's self assessment of subject knowledge is varied and inaccurate as trainees have no benchmark or reference point to based their skills assessment upon (Schmid 2021 1.1). Trainees lack the knowledge of the curriculum to judge if their stills are sufficient. (Jones et al., 2019) cited McNamara (1991) in stating; a teachers' restricted subject knowledge affects the quality of teaching and causes serious deficiencies in student learning, this deficiency is not immediately identifiable. (D2 V pedagogical research)

> Existing questionnaires have been criticized regarding the fuzzy, technology-unspecific, and content-agnostic wording of questionnaire items, which ask participants to rate the “appropriateness” of their competencies
(Schmid, Brianza and Petko, 2021)

Ultimately assessing subject knowledge for trainee teachers is a secondary concern. What matters is that they can deliver good lessons (V4 wider context). Subject knowledge could ultimately be assessed through the lesson content that the trainees create and deliver. This approach is confirmed by (Blömeke and Delaney, 2014) who state that we should be moving beyond self-reported subject knowledge audits. (K1 subject material)


### Action Research Question

Can trainee teachers accurately self-evaluate their subject knowledge and use this to guide their subject knowledge development?

### Context for the research

* Trainee Computing teachers need to accurately perform a gap analysis of their skill-set to self-guide their subject knowledge development over the duration of their one-year PGCE course. The purpose of the PGCE is to help trainees engage in continuous critical reflection.
* Mentors and trainees have expressed that; The existing subject knowledge audit is ineffective as it allow for personal interpretation of ability and typically measure the 'confidence' of the trainee.
* Intervention: Revise the computing subject knowledge audit
    * Remove the 'personal interpretation' from judgement of level of skill
* Identify how the trainee’s confidence is impacted by this new tool.
    * Will overly confident trainees be surprised or demoralised by the limits of their subject knowledge?


Literature Review and Rational for the Intervention
---------------------------------------------------

Developing teachers subject knowledge is a core component of Initial Teacher Training (Department for Education, “Initial Teacher Training (ITT): Core Content Framework” and “Teachers’ Standards”).
In 2016 Sentance and Csizmadia surveyed 300 Computing teachers in the UK (Sentance and Csizmadia). The biggest concern computing teachers had was "sufficient subject knowledge". (This research is ageing and may not reflect current trends)


### Definition of Computing Subject knowledge and skills

Denning, a professor of Computing working at MIT/Princeton and awarded "SIGCSE Award for Lifetime Service to Computer Science Education" has cautioned that
"Many employers no longer trust transcripts and diplomas.". He advocates that "computing education would benefit from a deep look at competency-based assessment". In his paper "Remaining trouble spots with computational thinking" he comparers "Traditional Computational Thinking" and "New Computational Thinking". Old computational thinking is traditional solving problems practically with developing programs on a computer. New computational thinking involves no use of a computer and thinking about problem solving approaches. New CT was coined by Wing (a professor of computer science at Carnegie Mellon) in 2006. Wing stated that these problem solving skills were not just relevant to computing could be used in other disciplines. 

Nijenhuis-Voogt et al did a qualitative study of 7 computing teachers in the netherlands about their approaches. Half of these teachers were coded as Old CT and the other half New CT. Although Kent is not the same setting, this tracks with my perceptions of what I observe in the attitudes of my school computing mentors in Kent. When I raised this point with my trainees in a seminar the debate seemed fairly balanced on both sides. These indicators combined gives me reasonable confidence in suggesting that there is general split between Old CT and New CT in the profession.

There have been debates between these two computational thinking ethos. Denning warns that new computational thinking has no evidence that indicates that the skills are transferable/useful in other domains. I agree with Denning. Young people build skills by writing real programs. New CT skills appear limited in scope in industry and limited scope as transferable skills.

Kolikant 2010 gives an argument for "make the CS classroom more authentic, more similar to the real work setting of CS practitioners" and "substituting lecture for lab time". 

Historically the GCSE has a non-exam assessed practical component. In 2018 Ofqual removed this because of mass nationwide cheating (Ofqual 2018). Trying to add practical components into schools sounds reasonable but Kolikant warns "Many times innovations get 'domesticated', are altered to fit the school grammar in a way that diminishes their change impact".

If New CT has little evidence to support it, then why is it so prevalent? I think this is due to the way computing is assessed in schools. The GCSE is 100% exam based without a computer. Theoretically a student can pass the exam without every touching a computer for the duration of their course. Schools rigorously focus their attention on training students to pass the exam. As GCSE's are the only real metric teachers are held accountable for, I can envisage that this has skewed teacher perception into thinking that New CT has greater weight.


### Existing Computing Audits

#### Expert Group: Computer Science Teacher Training (2012)

In 2012 the Department for Education released a document titled “Subject Knowledge Requirements for Entry into Computer Science Teacher Training. Expert Group’s Recommendations”.
> subject knowledge specified below is considered by an expert group ... to be the minimum necessary for trainees to take full advantage of the training offered and to produce teachers capable of teaching a rigorous course in the secondary phase

The content of this document is excellent. It covers many of the core aspects of computing including fetch-execute-cycle, data encoding, network protocols and error correction. Many of these straddle the boundary between GCSE and A-Level. 

If I was to actually use these criteria as "the minimum necessary" then my course would not run because I would have to reject most candidates at interview due to weak subject knowledge.

All the requirements state a trainee should "Explain"; there is no requirement for trainees to "Demonstrate" any of these skills.

#### Leeds Trinity University

* Categorised into 3 broad categories: Knowledge, Skills, and Learning Outcomes
* Items are hard to qualify:
    * "think creatively, innovatively, analytically, logically and critically"
    * use abstraction effectively
    * take a systematic approach to problem solving including the use of decomposition and abstraction, and make use of conventions including pseudo code and flowcharts
* Items are ranked with: (these are not well defined)
    1. I have some knowledge of this
    2. I have secure knowledge of this
    3. I can teach this
    4. I can teach this in several different ways

#### Sommerset Learning Platform

Somerset has a county learning platform that has Computing subject knowledge documents from 2015. The teacher knowledge requirements are a copy of the 'expert group recommendations from 2012' with little additional information about how these can be evidenced, achieved or tracked.

#### Survey other subjects audits

These are samples of subject knowledge audits for trainees teachers from Canterbury Christ Church University 2021/2022.

##### Maths

See Appendix XXX

Maths is a very established subject that challenges students problem solving abilities. This has parallels with the algorithmic problem solving in computing.

This is largely the gcse exam-board specification.

##### Music

See Appendix XXX

I selected Music as this subject has a range of demonstrable practical skills rather than desecrate knowledge. I wanted to compare how these practical skills are measured trainees.

Music Tech skills are listed separately. The categories are very broad (e.g Keyboard skills) with a rating 1 to 4. The rating descriptors are significantly lengthy as defined for the institution. Theses are the same descriptors I distilled my computing levels from.


#### Other measures of teacher knowledge

National Center for Computing Education (NCCE) have there own "Subject Knowledge Certificate" on a program called "The Computer Science Accelerator" (Computer Science Accelerator Programme 2019).

Subject knowledge certificates are awarded after attending multiple course modules (some online async, online live, or face-to-face) and completing a multiple choice assessment. Many of the questions are sourced from eedi.com's diagnostic questions bank. (Education Endowment Foundation 2021).

The NCCE have a set of Diagnostic Questions (Barton 2016) to assess teachers computing knowledge.

They provide a tool to 'map GCSE specifications to Computer Science Accelerator course'

Multiple choice questions only assess theory knowledge and not practical skill.


### Literature Review Conclusions

Audits for other subjects closely mirror the GCSE specification.

Existing audits target theory knowledge and often have vague criteria for ascertaining the current working level.

Practical application of computing skill is important. If young people can 'create a sound wave with code' they will understand how sound is stored and processed by a computer. If young person can 'send a UDP packet from one computer to another with code' then they understand network addresses, ports and sockets. Many of the national curriculums theory requirements can be explained meaningfully though practical application. I expect a computing teachers skill-set to reflect this.


Proposed New Audit
------------------

My re-worked subject knowledge audit proposes a new level system:
1. Theoretical understanding (evidence - maybe past exam question or previous degree module grade)
2. Have a practical implementation on a computer (evidence - link to github code? screenshot?)
3. Have taught (evidence resources created or lesson plans)
4. Can engage in Masters level critical reflection about multiple ways of teaching the topic

My rational is: If you can't write a bubble-sort algorithm with code, how can you teach it? Practical implementation should be a prerequisite for teaching an engaging computing lesson. 

The new audit will be based on A-Level (KS5) as a progression model. It is a superset of KS3 and KS4.

A copy of this experimental audit is available in Appendix XXX



Research Methodology
--------------------

Rework the PGCE trainee teacher computing subject knowledge audit to include measurable practical implementation of theory skills.


### Aims and objectives
* Aims (statement of intent)
    * Trainees should be able to autonomously perform a subject knowledge gap analysis
    * Trainees should target and evidence subject knowledge development over a 3-month period
* Objectives (actions and measurable outcomes)
    * Ensure that levels of subject knowledge are unambiguous and evidencable
    * Trainee teachers and school mentors to accurately identify their current working level and areas of need
    * Provide a quantitative measure of subject knowledge progress over a 3-month period
    * Trainees should be able to self direct their subject knowledge development

### Intervention
1. Rework the existing 'computing trainee teacher subject knowledge audit'
    * Simplify and remove ambiguity in the criteria for identifying subject knowledge proficiency level
    * Clarify the evidence required to track/justify subject knowledge
    * Survey trainee teachers and school mentors about their perceptions of the new structure for subject knowledge tracking to ascertain effectiveness of the intervention
2. Trainees and mentors’ responses will be captured to identify the effectiveness of this rework
    * Survey trainee’s perceived confidence of their subject knowledge development

### Criteria for evaluation

Key measures indicating success:
1. There is majority preference for the new audit model
    1. From students
    2. From mentors
2. Changes to student confidence levels should remain minimal
    * It should neither strongly inflate or deflate their ego artificially
3. Students feel they can accurately justify their current level of subject knowledge from the audit
4. There is evidence to support that practical computing skills development has taken place
    * This could be evidenced by student portfolio progress when reviewed in December


### Research Design


#### Questionnaires

I will give my trainees two questionnaires (3 months apparat) that survey their experience with the new subject knowledge audit. I will attempt to codify the responses to the open questions. I will use a unipolar likert scale to quantify overarching conclusions for preference for the old/new audit and student self reported confidence.

#### Codification of open question responses

Action Research Education (Mcater 2013) proposes a range of approaches for research techniques in education. I will attempt to perform a thematic analysis by  identifying unexpected data and points of disagreements and codifying these outcomes. I should be cautions about the seduction of dominant discourse. "Modes of discourse become so culturally embedded that they enter into widespread acceptance, and are rarely the subject of question or debate" (Mcateer and British Educational Research Association 107–128 (Chapter 6))

I have a sample size of 7 students and mentors, my study will be of poor quality based on the quality indicators (Reliability, validity, replicable, objectivity, generalisability).

Bridges 2007 warns: "At it's best, classroom action research seems to me to represent a reaffirmation of personal integrity, responsibility and authority in an environment that threatens to undermine all of these". Action research's main use is for a practitioner to attempting to realign their thinking.

The 'Coding Manual For Qualitative Researchers' (Saldaña 2016) cites an argument from Strauss's conveying the importance of coding in qualitative research. 
This is counted by an assertion from Packer that on a practical level, coding is impossible to practice.

(Saldaña 2016) describes first cycle coding methods and second cycle coding methods for taking open response questions and categorising them. In professional research this codification should be done by someone external to the study or the domain to reduce bias. I will attempt to perform a cut down rendition of these principles.

First cycle, I will identify the main themes and give them a code. Second cycle, I wil attempt to count the number of occurrences of these codes. I will demonstrate the codes and commentary in my appendix.

This approach is a significantly simplified attempt to interpret my data. This is an area I would like to formalise/develop as I continue my journey with education research.


#### Focus group discussion

Pinar (2011) in the book "What Is Curriculum Theory?" defined the curriculum as "complicated conversations" with practitioners rather than the formulation of objectives evaluated by standardized tests. Pinar posits that the curriculum is largely intertwined with the perceptions of the practitioners that teach it. 

I will record the audio from a focus group conversation between my trainees after 3 months of using the new subject knowledge audit. This open forum will hopefully reveal deeper insights than my questionnaires as this will involve discourse between eh trainee teachers. They will be asked to discuss the same questions given to mentors.

I will attempt to perform an Interpretative phenomenological analysis (IPA) to offer insight into to offer insights into how trainee teachers computing teachers makes sense of a subject knowledge development. This is in the domain of Hermeneutics as the theory and practice of interpretation where the interpretation can be justified.

The small sample size of trainees makes this approach possible.

I will use the same codification process as used with the questionnaires to interpret the focus group discussion.


#### Ethical considerations of research design

This study will alter the way trainees evaluate their subject knowledge. This self identification could impact their personal identity. Given the new audit refers to more practical skills, it is possible that confidence of my trainees could be affected. To mitigate this I will:
* Ensure the audit clearly describes that it "is not used for formal assessment"
* Ensure I collect feedback about the trainees level of confidence - both lichert scale and free-from feedback to detect any issues
* Continually involve trainees in discussions about subject knowledge development to support them

No changes need to be made to the course in general and no change to assessment or re-validation will be required.

All participants have completed consent forms and are aware that their data is collected anonymously so they will not be identifiable, but their responses will be publicly visible.

Two questionnaires will be given 3 months aspart. To match the trainees responses over the time period, rather than collecting a name I a collect a 'code-name' they can enter in each questionnaire to identify participants.


Data Acquisition
----------------

See Appendix XXX for questionnaire's and focus group questions.


Results
-------

I have totalled the codified sentiments and discussed them below.
See Appendix for full results codification table. 

I was unable to track individual progress from september to december. The questionnaire were anonymous and students forgot the fake-names/identifiers that had used in the first questionnaire. (I need a new strategy for this in future)


### Results - Codification - Commentary

I will work though the major points from the codified responses giving commentary.


#### Scope2 - Topics out of alignment with teaching - 18 references

Universally trainees and mentors voiced that the new subject knowledge audit did not align with the schools curriculum.

It is worth noting that at this point trainees had been in school a total of 32 days (8 weeks * 4 days).

> a lot of areas that have not changed as they have not been relevant to what I have been teaching.

> ... there are several points that would not be covered in a Secondary School

> it adds an unnecessary overhead of a student teacher going to learn a specific point of this tracker which whilst may be useful knowledge will not be applied to the classroom setting

> the new subject knowledge audit does not meet some of the topics I have made lesson ... Basic construct in programming (Sequence, Selection and Iteration)

These 'Basic constructs' described are lower level skills and were not listed. Low level skills are inferred by the higher level skills. To write simple algorithms (like a bubble sort) a teacher needs these 'Base constructs'. This overlaps with Misunderstanding3 as this is 'a teacher skill audit' and not 'a student skill audit'. This would metaphorically be similar to listing basic skills in other domains such as explicitly listing addition/multiplication for maths.

> It should feel like it all fits together to tick off your GCSE lesson. 

The audit was meant to convey the whole 'world map' of the skills a computing teacher. Trainees did not want/need an overview of the discipline, they wanted a 'tick list' for the year. This highlights that my goals are out of alignment with my learners goals.


#### Quantifiable1 - New audit quantifiable/measurable/specific - 11 references

Trainees felt the new levelling system allowed them to clearly identify their current level of knowledge for each item. However this point is largely irreverent as stated above with S2, these skills were out of alignment with the skills trainees needed.

> I still prefer the new audit model for its clarity and easy to answer the confidence for each topic


#### Misunderstandings(1-7) - 15 references

Trainees misunderstood the audit was a "for teachers knowledge" and not "tasks for students". This created tension between the trainees, mentors and damaged rapport with the mentors.

> I do worry that learning some of these topics will require me to spend time away from lesson planning for the students with who I am working in order to fit these topics into my classes

> difficulties of showing students completed work ... for many of these categories.

> The department management outright refused to allow me to teach according to this audit

Another misunderstanding was trainees believed they had to complete the whole audit.

> I would be unable to complete everything on the audit

> I haven't had time for significant work on my subject knowledge which is outside areas of the curriculum


#### Scope6 - Request more macro scope - 4 references

Some trainees requested broader scope in the items citing "Societal Impact" and "Privacy". My concern with this broad definitions is how would a trainee be able to measure their knowledge in these areas? They are so ephemeral I don't think they would provide value in an audit.

Trainees wanted more autonomy over their skill development.

> (old audit) gives me more flexibility to work on developing my skills in ways that I see fit, as opposed to a pre-prescribed way


### Scale questions

### Now you have used the new subject audit model. How do you feel about the previous and new models?
|Response|Count|
|-|-|
|I prefer the new audit model|1|
|I somewhat prefer the new audit model|2|
|I have no preference|0|
|I somewhat prefer the previous audit model|2|
|I prefer the previous audit model|1|

Conclusion: Ambivalent between old and new audits - trainees largely ignored the audit


### How do you currently feel about your current skill-set as described by the subject knowledge audit gap analysis?
|Response|Count|
|-|-|
|I am slightly under-confident about my abilities|3|
|I have no response|1|
|I feel slightly confident about my abilities|2|

Conclusion: Little impact on confidence - trainees largely ignored the audit



Conclusions
-----------

The old subject knowledge audit provided little meaningful value because the categories were not well defined enough for a novice practitioner to identify areas of development

The new subject knowledge audit provided little meaningful value because the categories were not aligned with the institutions novice practitioners were teaching in.

My original concept, was that if teachers were able to practically demonstrate the theoretical computing concepts with practical application. This would improve the quality of lessons computing teachers were able to create. The average trainee does not have the practical skills to do this and there is not sufficient time within the course to develop these skills. In modifying to the subject knowledge audit to include a practical dimension I alienated mentors and demoralised trainees.

I wanted to show a 'complete world map' of the dimensions of the discipline of computing. Even with verbal guidance and supporting text, many of my trainees were unable to distinguish what 'could be possible over a lifetime of teaching computing' with 'what is required in the next few months to gain qualified teacher status'. They were unable to conceive the audit tool as something they might be referring to for the next 5 years+. This created confusion and stress for trainees.

As the audits were not an assessment requirement, it was largely ignored by trainees and mentors.


### Reflecting on effectiveness of intervention

* Objectives (actions and measurable outcomes)
    * Ensure that levels of subject knowledge are unambiguous and evidencable
        * Somewhat met: Trainees agreed the new levels reduced ambiguity
    * Trainee teachers and school mentors to accurately identify their current working level and areas of need
        * Not met: Scope of new audit was outside of GCSE requirements
    * Provide a quantitative measure of subject knowledge progress over a 3-month period
        * Not met: Trainee teachers did not have time to focus on any activity not related directly to classroom teaching. Little to no progress on audit skills
    * Trainees should be able to self direct their subject knowledge development
        * Not relevant: Trainees largely ignored the new audit and only focused on immediate lesson planning

### Reflecting on key measures of success

1. There is majority preference for the new audit model
    * Not met: students did not use the old or new audits and were largely ambivalent to both
2. Changes to student confidence levels should remain minimal
    * Not related to intervention: most trainees ignored the audit as it was not an assessment requirement
3. Students feel they can accurately justify their current level of subject knowledge from the audit
    * Not met: trainees largely ignored the audit
4. There is evidence to support that practical computing skills development has taken place
    * Not met: trainees were constrained (especially at this stage in the course), to teach the lessons provided by their placement school. Skill development was evidenced by the lessons that they were guided to teach.

### Reflecting on research questions

* Research Question 1: "Can trainee teachers accurately self-evaluate their subject knowledge"
    * "yes" given clear enough criteria.
* Research Question 2: "and use this to guide their subject knowledge development?"
    * The reality is "no"
    * The driver for trainee teacher subject knowledge development is largely coupled to the trainees placement school curriculum. Anything beyond this is pragmatically optional. Trainees entire focus is delivering the content required by their placement school and learning the fundamentals of how to teach. Additional subject knowledge development is not assessed and therefore not actioned.

### Reflecting on the Scope of the school computing curriculum

Trainees cited many times "not on the computing curriculum". In a subject session I asked them "was AI on the curriculum". The were unable to cite where it was. Their answer was "AI was not on the curriculum".
The national curriculum is deliberately open for interpretation. Each school has the freedom form their own approach. 
Professor Simeon Peyton Jones cited in his NCCE address "Where is the AI". He states "we don't need to rewrite the national curriculum to include AI. AI is already inferred by the opening paragraph" (Peyton Jones 2021)
* > A high-quality computing education equips pupils to use computational thinking and creativity to understand and change the world
* > ... how digital systems work and how to put this knowledge to use through programming
* > ... at a level suitable for the future workplace and as active participants in a digital world
* > ... and have repeated practical experience of writing computer programs in order to solve such problems
AI is part of our world and we should be teaching it to our young people.
There are many freely available web accessible systems for teaching AI to whole class's of KS2, KS3 and KS4 students.

The 7 trainees in this survey had only 32 days of school experience. They have limited experience to make judgements about curriculum content.

KS3 is flexible and at the schools discretion. KS4 is examined by a national paper exam. To raise results, schools ensure KS3 has a progression pathway of the concepts in KS4. Thus leading to the prevailing opinion that many topics are "not in the curriculum" if there is not a direct exam question for them.


### Next Study: Suggestions

Biggs and Tang 2011 describe the concept of "Constructive Alignment" where the learning objectives, assessment and lesson content are all in alignment. They suggest that any change to one of these must involve changes to all three components. By changing only one aspect of my course it was clear that it was going to have limited impact and create confusion. Future studies that involve alterations to course materials should be carefully consider other aspects that need to be updated.

For a more academically rigorous study, it would be wise to ensure that someone external to the project provides the codification of questionnaire responses or interviews. This will counter some the inherent bias that the creators of the study may have. Interviews should also be conducted by 3rd parties for the same reason.

To get a complete picture of teacher skills development, the scope should be widened to study teachers with a range of years of service. This could reveal deeper insights into how teachers skills develop to inform meaningful changes to initial teacher education.


### Actions to improve practice

My subject knowledge audit enhancements focused around developing practical computing skills. The reality is that students (and therefore teachers) don't need these practical skills. The requirement for teachers is that their students have the knowledge to pass the GCSE exam.

There is no need for a separate specialised teacher subject knowledge audit.
An exam board GCSE specification lists the knowledge/skills needed and has mechanisms for assessment/measurement.
Trainees teachers do not have the capacity to focus on developing sills for topics that their students are not directly assessed on.

Is up to the individual teachers or individual school computing departments to develop their own custom computing opportunities (if they have the capacity). These additional opportunities cannot (and probably 'should not') be controlled by external entities.

The government has provided £80 million funding for the NCCE upskill computer science teachers. They are offering nationally accredited courses and bursary's of £2500 for trainees to take to their first employing school. This gives trainees incentive and accreditation for their subject knowledge development. This funding is due to expire in the summer of 2022 (Sentance 2019). It will be interesting to see what incentives are offered to computing trainees in future.

Actions for my teaching next academic year 2022/2023:
* Replace the 'trainee teacher subject knowledge audit' with an amalgamation of current GCSE specifications from active exam boards
    * NCCE (“Computer Science Accelerator Programme”) provides a mapping tool for exam board specification to available courses
* Ask student teachers to judge their level of skill by:
    * Self administering and marking their own attempt at GCSE past papers
    * Using existing NCCE self assessment tooling. (“Diagnostic and Summative Assessment of Computing”) online tool
* Remove any reference to A-Level topics - these are beyond the scope of the course
* Direct trainee teachers towards existing skills certifications NCCE (“Computer Science Accelerator Programme”)
    * By doing this trainee teachers accrue bursaries to take to their first employing school. This manifests externally visible and verifiable progress
* Give trainee teachers the opportunity to sit a real GCSE computing exam in a school and be graded
    * (In reality this should be a mandatory requirement of any trainee teacher in any subject)


### What next?

Industry is taking measures into their own hands. Many employers are loosing faith in the weight behind formal computing qualifications. Industry is inventing their own professional certification and their own recruitment screening process that is largely based on practical skills. (Denning, more citation needed, this is common)

A follow-up questions might be: "What is the purpose of Computing Education in secondary schools", "What should schools be focusing on?" and "How can we make meaningful positive change to computing education?".

Speaking to undergraduates and local school teachers, there is a growing desire to develop practical computing skills that formal education is not catering for. Coder Dojo's (coderdojo.com), Coding Dojo (codingdojo.org), Code Club's (codeclub.org) and other independent groups are setting up all over the world to cater for the missing parts of computing from mainstream education. I am in the process of investigating how to provide weekly group that spans secondary students, though undergrads, though junior developers. As an institution, Canterbury Christ Church is in a great position to host such a group. It would be interesting to see if this approach is viable and will be the focus of my next investigation.

Bibliography
============

Barton, Craig. “Diagnostic Questions: How To...” Mr Barton Maths Blog, 2016, www.mrbartonmaths.com/blog/diagnostic-questions/. Accessed 22 Jan. 2022.

Biggs, John B, and Catherine So-Kum Tang. Teaching for Quality Learning at University : What the Student Does. Maidenhead, England ; New York, Mcgraw-Hill, Society For Research Into Higher Education & Open University Press, 2011.

Blömeke, Sigrid, and Séan Delaney. “Assessment of Teacher Knowledge across Countries: A Review of the State of Research.” International Perspectives on Teacher Knowledge, Beliefs and Opportunities to Learn, 2014, pp. 541–585, 10.1007/978-94-007-6437-8_25.

Christian, Brian, and Tom Griffiths. Algorithms to Live by : The Computer Science of Human Decisions. New York, Henry Holt And Company, 2016.

“Computer Science Accelerator Programme.” Teach Computing, National Center for Computing Education, teachcomputing.org/cs-accelerator. Accessed 2019. Subject knowledge certificate.

Connell, Andrew, et al. A Practical Guide to Teaching Computing and ICT in the Secondary School. Abingdon, Oxon ; New York, Ny, Routledge, 2015. Chapter 1: Developing your capability to teach Computing. Gavin Rhoades pg 10.

Denning, Peter J. “Remaining Trouble Spots with Computational Thinking.” Communications of the ACM, vol. 60, no. 6, 24 May 2017, pp. 33–39, 10.1145/2998438. Accessed 14 Nov. 2019.

Department for Education. “Initial Teacher Training (ITT): Core Content Framework.” GOV.UK, 1 Nov. 2019, www.gov.uk/government/publications/initial-teacher-training-itt-core-content-framework.

“Teachers’ Standards.” GOV.UK, Department for Education, 1 July 2011, www.gov.uk/government/publications/teachers-standards.

“Diagnostic and Summative Assessment of Computing.” Eedi.com, National Center for Computing Education, eedi.com/projects/teach-computing. Accessed 15 Jan. 2022.

Education Endowment Foundation. “Eedi: Testing an Online Assessment Tool That Helps Maths Teachers to Identify and Address Pupil Misconceptions.” Educationendowmentfoundation.org.uk, Mar. 2021, educationendowmentfoundation.org.uk/projects-and-evaluation/projects/diagnostic-questions. Accessed 22 Jan. 2022.

Jones, Lewis C. R., et al. “The Effect of Teacher’s Confidence on Technology and Engineering Curriculum Provision.” International Journal of Technology and Design Education, 7 Aug. 2019, 10.1007/s10798-019-09542-4. Accessed 27 Oct. 2020.

Koehler, Matthew J, and Punya Mishra. Handbook of Technological Pedagogical Content Knowledge (TPCK) for Educators. Edited by Mary C. Herring and The AACTE Committee on Innovation and Technology, Routledge, 11 June 2014, doi.org/10.4324/9781315759630. Accessed 10 Aug. 2021.

Kolikant, Y. Ben-David. “Innovative Teaching in Computer Science: What Does It Mean and Why Do We Need It?” Computer Science Education, vol. 20, no. 2, June 2010, pp. 73–78, 10.1080/08993408.2010.486239. Accessed 4 June 2019.

Mcateer, Mary, and British Educational Research Association. Action Research in Education. London, Sage Publications Ltd, 2013, pp. 107–128 (Chapter 6), www.vlebooks.com/Product/Index/323535. Accessed 9 Jan. 2022.

Nijenhuis-Voogt, Jacqueline, et al. “Teaching Algorithms in Upper Secondary Education: A Study of Teachers’ Pedagogical Content Knowledge.” Computer Science Education, 15 June 2021, pp. 1–33, 10.1080/08993408.2021.1935554. Accessed 8 Jan. 2022.

Ofqual. “Revised Arrangements for GCSE Computer Science.” GOV.UK, 8 Jan. 2018, www.gov.uk/government/news/revised-arrangements-for-gcse-computer-science. Accessed 10 Jan. 2022.

Peyton Jones, Simon. “NCCE Digital Skills Forum - AI and the Future of Education.” Www.youtube.com, National Centre for Computing Education, 17 June 2021, youtu.be/tvtJdi8u7vU?t=215. Accessed 8 Jan. 2022.

Pinar, William F. What Is Curriculum Theory? London, Routledge, 2011.

Pu, Song, et al. “Factors Affecting Practical Knowledge Acquisi-Tion of Pre-Service Computer Science Teachers during the Practicum: A Multiple Regression Analysis.” International Journal of Learning, Teaching and Educational Research, vol. 19, no. 2, 28 Feb. 2020, pp. 214–230, 10.26803/ijlter.19.2.13. Accessed 10 Aug. 2021.

Saldana, Johnny. Coding Manual for Qualitative Researchers. Fourth Edition ed., S.L., Sage Publications, 2021.

Schmid, Mirjam, et al. “Self-Reported Technological Pedagogical Content Knowledge (TPACK) of Pre-Service Teachers in Relation to Digital Technology Use in Lesson Plans.” Computers in Human Behavior, vol. 115, Feb. 2021, p. 106586, 10.1016/j.chb.2020.106586. Accessed 29 July 2021.

“Secondary PGCE Interview Downloads.” Leeds Trinity University, www.leedstrinity.ac.uk/study/teaching/routes/university-led-route/pgce-secondary/secondary-education-interview-days/secondary-pgce-interview-downloads/. Accessed 15 Jan. 2022. PGCE Computer Science Subject Knowledge Audit.

Sentance, Sue. “Moving to Mainstream.” Proceedings of the 14th Workshop in Primary and Secondary Computing Education, 23 Oct. 2019, 10.1145/3361721.3362117.

Sentance, Sue, and Andrew Csizmadia. “Computing in the Curriculum: Challenges and Strategies from a Teacher’s Perspective.” Education and Information Technologies, vol. 22, no. 2, 5 Apr. 2016, pp. 469–495, link.springer.com/article/10.1007%2Fs10639-016-9482-0, 10.1007/s10639-016-9482-0.

“Subject Knowledge Requirements for Entry into Computer Science Teacher Training. Expert Group’s Recommendations.” Digital Education Resource Archive (DERA) - University College London: Institute of Education, Teaching Agency: Department for Education, 12 July 2012, dera.ioe.ac.uk/15780/1/subject%20knowledge%20requirements%20for%20entry%20into%20cs%20teacher%20training.pdf. Accessed 15 Jan. 2022.

Thorsnes, Jørgen, et al. “In-Service Teacher Training and Self-Efficacy.” Informatics in Schools. Engaging Learners in Computational Thinking, 2020, pp. 158–169, 10.1007/978-3-030-63212-0_13. Accessed 10 Aug. 2021.

Wing, Jeannette M. “Computational Thinking.” Communications of the ACM, vol. 49, no. 3, 1 Mar. 2006, p. 33, dl.acm.org/citation.cfm?id=1118215, 10.1145/1118178.1118215.

Woodard, Jared. “Rotten STEM: How Technology Corrupts Education.” American Affairs Journal, American Affairs Foundation, 20 Aug. 2019, americanaffairsjournal.org/2019/08/rotten-stem-how-technology-corrupts-education/. Accessed 8 Jan. 2022. Fall 2019 / Volume III, Number 3.


Appendix
========

@import "./prelude.md"

@import "../data-collection/students-december.csv"

@import "../data-collection/coding.md"