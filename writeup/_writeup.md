# Developing an effective tool to enable trainee teachers to perform an accurate gap analysis of their existing skill-set

Abstract
--------

As a Teacher Educator, I want to improve trainee teachers pedagogical development. Trainee teachers are required to show evidence of the their subject knowledge development to meet the UK teachers' standards.

Explain Why this come to attention


Overview
--------

Background as a secondary teacher, senior software engineer, teacher educator



### Action Research Question
Can trainee teachers accurately self-evaluate their subject knowledge and use this to guide their subject knowledge development?

* An appropriate range of relevant issues identified and distinctive approach taken in dealing with them.

??? aims/ objectives / intervention?

### Context for the research
* Trainee Computing teachers need to accurately perform a gap analysis of their skill-set to self-guide their subject knowledge development over the duration of their one-year PGCE course. The purpose of the PGCE is to help trainees engage in continuous critical reflection.
* Mentors and trainees have expressed that; The existing subject knowledge audit is ineffective as it allow for personal interpretation of ability and typically measure the 'confidence' of the trainee.
* Intervention: Revise the computing subject knowledge audit
    * Remove the 'personal interpretation' from judgement of level of skill
* Identify how the trainee’s confidence is impacted by this new tool.
    * Will overly confident trainees be surprised or demoralised by the limits of their subject knowledge?



Literature Review and Rational for the Intervention
---------------------------------------------------

Developing teachers subject knowledge is a core component of Initial Teacher Training (Department for Education, “Initial Teacher Training (ITT): Core Content Framework” and “Teachers’ Standards”).
In 2016 Sentance and Csizmadia surveyed 300 Computing teachers in the UK (Sentance and Csizmadia). The biggest concern computing teachers had was "sufficient subject knowledge". 2016 was two years after the new computing curriculum for schools was introduced (2014). I suspect that in 2022 this is likely to have changed.


### Enhancing Teaching, Assessing and Supporting HE (ETAS) - Question

I wrote a short piece earlier this year raising question the issues about supporting teachers subject knowledge development in computing. A few important excerpts are listed below:

Students are required to self assess their skills and identify their subject knowledge gaps (Rhoades 2014). Rhoades states "any overestimation or even underestimation of your knowledge will be counter productive". Trainee's self assessment of subject knowledge is varied and inaccurate as trainees have no benchmark or reference point to based their skills assessment upon (Schmid 2021 1.1). Trainees lack the knowledge of the curriculum to judge if their stills are sufficient. (Jones et al., 2019) cited McNamara (1991) in stating; a teachers' restricted subject knowledge affects the quality of teaching and causes serious deficiencies in student learning, this deficiency is not immediately identifiable. (D2 V pedagogical research)

> Existing questionnaires have been criticized regarding the fuzzy, technology-unspecific, and content-agnostic wording of questionnaire items, which ask participants to rate the “appropriateness” of their competencies
(Schmid, Brianza and Petko, 2021)

Ultimately assessing subject knowledge for trainee teachers is a secondary concern. What matters is that they can deliver good lessons (V4 wider context). Subject knowledge could ultimately be assessed through the lesson content that the trainees create and deliver. This approach is confirmed by (Blömeke and Delaney, 2014) who state that we should be moving beyond self-reported subject knowledge audits. (K1 subject material)


### Definition of Computing Subject knowledge and skills

Denning, a professor of Computing working at MIT/Princeton and awarded "SIGCSE Award for Lifetime Service to Computer Science Education" has cautioned that
"Many employers no longer trust transcripts and diplomas.". He advocates that "computing education would benefit from a deep look at competency-based assessment". In his paper "Remaining trouble spots with computational thinking" he comparers "Traditional Computational Thinking" and "New Computational Thinking". Old computational thinking is traditional solving problems practically with developing programs on a computer. New computational thinking involves no use of a computer and thinking about problem solving approaches. New CT was coined by Wing (a professor of computer science at Carnegie Mellon) in 2006. Wing stated that these problem solving skills were not just relevant to computing could be used in other disciplines. 

Nijenhuis-Voogt et al did a qualitative study of 7 computing teachers in the netherlands about their approaches. Half of these teachers were coded as Old CT and the other half New CT. Although Kent is not the same setting, this tracks with my perceptions of what I observe in the attitudes of my school computing mentors in Kent. When I raised this point with my trainees in a seminar the debate seemed fairly balanced on both sides. These indicators combined gives me reasonable confidence in suggesting that there is general split between Old CT and New CT in the profession.

There have been debates between these computational thinking two ethos. Denning warns that new computational thinking has no evidence that indicates that the skills are transferable/useful in other domains. I attempted to find evidence for the impact of New CT in other disciplines. With my limited searching I could find any measurable evidence. I agree with Denning. Unless young people build skills by writing real programs, these softer new problem solving skills without a computer appear to have little meaningful value. New CT skills appear limited in scope in industry and limited scope as transferable skills.

Christian and Griffiths in their book Algorithms to Live by: The Computer Science of Human Decisions 2016, attempts to describe computer science patterns that map onto human behaviour. These are often trivial observations e.g. "The use of caching" by keeping regularly used objects in more accessible locations. A deep knowledge of computing does not seem to offer a meaningful advantage or deeper insight into how people engage with everyday activities.

Kolikant 2010 gives an argument for "make the CS classroom more authentic, more similar to the real work setting of CS practitioners" and "substituting lecture for lab time". Historically was a non-exam assessed practical component for GCSE, but in 2018 Ofqual removed this because of mass nationwide cheating (Ofqual 2018). Trying to add practical components into schools sounds reasonable but Kolikant warns "Many times innovations get 'domesticated', are altered to fit the school grammar in a way that diminishes their change impact".

If New CT has little evidence to support it, then why is it so prevalent? I think this is due to the way computing is assessed in schools. The GCSE is 100% exam based without a computer. Theoretically a student can pass the exam without every touching a computer for the duration of their course. Schools rigorously focus their attention on training students to pass the exam. As GCSE's are the only real metric teachers are held accountable for, I can envisage that this has skewed teacher perception into thinking that New CT has greater weight.

Woodard in his American Affairs opinion piece "Rotten STEM" cautions that STEM is no longer about Science and Maths and is all about providing tech workers for industry. We should be cautions about the requirements of industry contaminating education. Education is border than just developing monetisable skills (Woodard 2019). It may not be wise to map computing education exclusively on to industry requirements.

My preference is that I favour practical application of skill. If young people can create a sound wave with code they will understand how sound is stored and processed by a computer. If young person can send a TCP packet from one computer to another with code then they understand network addresses, ports and sockets. Many of the national curriculums theory requirements can be explained meaningfully though practical application. I expect a computing teachers skill-set to reflect this.



### Other Computing Audits

#### Expert Group: Computer Science Teacher Training (2012)

In 2012 the Department for Education released a document titled “Subject Knowledge Requirements for Entry into Computer Science Teacher Training. Expert Group’s Recommendations”.
> subject knowledge specified below is considered by an expert group ... to be the minimum necessary for trainees to take full advantage of the training offered and to produce teachers capable of teaching a rigorous course in the secondary phase

The content of this document is excellent. It covers many of the core aspects of computing including fetch-execute-cycle, data encoding, network protocols and error correction. Many of these straddle the boundary between GCSE and A-Level. If I was to actually use these criteria as "the minimum necessary" then my course would not run because I would have to reject candidates at interview for subject knowledge.

All the requirements state a trainee should "Explain"; there is no requirement for trainees to "Demonstrate" any of these skills.

#### Leeds Trinity University

* Categorised into 3 broad categories: Knowledge, Skills, and Learning Outcomes
* Items are hard to qualify:
    * "think creatively, innovatively, analytically, logically and critically"
    * use abstraction effectively
    * take a systematic approach to problem solving including the use of decomposition and abstraction, and make use of conventions including pseudo code and flowcharts
* Items are ranked with: (these are not well defined)
    1. I have some knowledge of this
    2. I have secure knowledge of this
    3. I can teach this
    4. I can teach this in several different ways

#### Sommerset Learning Platform

Somerset has a county learning platform that has Computing subject knowledge documents from 2015. The teacher knowledge requirements are a copy of the expert group recommendations with little additional information about how these can be evidenced, achieved or tracked.

#### Survey other subjects audits

These are samples of subject knowledge audits for trainees teachers from Canterbury Christ Church University 2021/2022.

##### Maths

See Appendix XXX

Maths is a very established subject that challenges students problem solving abilities. This has parallels with the algorithmic problem solving in computing.

This is largely the gcse exam-board specification.

##### Music

See Appendix XXX

I selected Music as this subject has a range of demonstrable practical skills rather than desecrate knowledge. I wanted to compare how these practical skills are measured trainees.

Music Tech skills are listed separately. The categories are very broad (e.g Keyboard skills) with a rating 1 to 4. The rating descriptors are significantly lengthy as defined for the institution. Theses are the same descriptors I distilled my computing levels from.


#### Survey of external audits

In 2016 Craig Barton, a maths teacher, demonstrated the value of multiple choice questions where the incorrect answers are deliberately constructed to reveal a particular misconception. The effectiveness was evaluated by a 3 year project by the Education Endowment Foundation. The technique has also been used for testing computing subject knowledge. (“Diagnostic and Summative Assessment of Computing”)
> Gaining an accurate picture of the subject knowledge of your students, and knowing where gaps remain, provides the foundation for success. 

[Subject knowledge certificate](https://teachcomputing.org/cs-accelerator)
> Complete a questionnaire to assess your subject knowledge
[GCSE specifications to Computer Science Accelerator course map (September 2020)](https://static.teachcomputing.org/GCSE_specifications_to_CSA_course_map.pdf)

Subject knowledge certificates are awarded after attending a course module (some online async, online live, or face-to-face) and completing a multiple choice assessment with many of the questions mirrors from eedi.com's diagnostic questions.


### Literature Review Conclusions

lack of practical evidence in subject knowlge audits

What are the research questions ...

..? If you can't write a bubblesort algorithum with code, how can you teach it? .. it's a prerequestit. Other subjects do not seem to identify this practical requirement

A-level by practical
If we take A-Level as a progression model - it is a superset of KS3 and KS4



Research Methodology
--------------------

### The intervention

Rework the PGCE trainee teacher computing subject knowledge audit to include measurable practical implementation of theory skills.


### Aims and objectives
* Aims (statement of intent)
    * Trainees should be able to autonomously perform a subject knowledge gap analysis
    * Trainees should target and evidence subject knowledge development over a 3-month period
* Objectives (actions and measurable outcomes)
    * Ensure that levels of subject knowledge are unambiguous and evidencable
    * Trainee teachers and school mentors to accurately identify their current working level and areas of need
    * Provide a quantitative measure of subject knowledge progress over a 3-month period
    * Trainees should be able to self direct their subject knowledge development

### Intervention
1. Rework the existing 'computing trainee teacher subject knowledge audit'
    * Simplify and remove ambiguity in the criteria for identifying subject knowledge proficiency level
    * Clarify the evidence required to track/justify subject knowledge
    * Survey trainee teachers and school mentors about their perceptions of the new structure for subject knowledge tracking to ascertain effectiveness of the intervention
2. Trainees and mentors’ responses will be captured to identify the effectiveness of this rework
    * Survey trainee’s perceived confidence of their subject knowledge development

### Criteria for evaluation

Key measures indicating success:
1. There is majority preference for the new audit model
    1. From students
    2. From mentors
2. Changes to student confidence levels should remain minimal
    * It should neither strongly inflate or deflate their ego artificially
3. Students feel they can accurately justify their current level of subject knowledge from the audit
4. There is evidence to support that practical computing skills development has taken place
    * This could be evidenced by student portfolio progress when reviewed in December


### Research Design


#### Questionnaires

I will give my trainees two questionnaires (3 months apparat) that survey their experience with the new subject knowledge audit. I will attempt to codify the responses to the open questions. I will use a unipolar likert scale to quantify overarching conclusions for preference for the old/new audit and student self reported confidence.

#### Codification of open question responses

Action Research Education (Mcater 2013) proposes a range of approaches for research techniques in education. I will attempt to perform a thematic analysis by  identifying unexpected data and points of disagreements and codifying these outcomes. I should be cautions about the seduction of dominant discourse. "Modes of discourse become so culturally embedded that they enter into widespread acceptance, and are rarely the subject of question or debate" (Mcateer and British Educational Research Association 107–128 (Chapter 6))

I have a sample size of 7 students and mentors, my study is limited in most of the data quality indicators (Reliability, validity, replicable, objectivity, generalisability). I intend to extract the general themes that arise and discuss them.

Bridges 2007 warns: "At it's best, classroom action research seems to me to represent a reaffirmation of personal integrity, responsibility and authority in an environment that threatens to undermine all of these". Action research's main use is for a practitioner to attempting to realign their thinking.

In Coding Manual For Qualitative Researchers (Saldaña 2016) cites an argument from Strauss's conveying the importance of coding in qualitative research. 
This is counted by an assertion from Packer that on a practical level, coding is impossible to practice. 
The book describes first cycle coding methods and second cycle coding methods for taking open response questions and categorising them. In professional research this codification should be done by someone external to the study or the domain to reduce bias. I will attempt to perform a cut down rendition of these principles.

First cycle, I will identify the main themes and give them a code. Second cycle, I wil attempt to count the number of occurrences of these codes. Because my sample size is so small I will simply this further with reporting the code of themes that have occurred multiple times. I will demonstrate the codes and commentary in my appendix.

This approach is a significantly simplified attempt to interpret my data. This is an area I would like to formalise/develop as I continue my journey with education research.


#### Focus group discussion

Pinar (2011) in his book "What Is Curriculum Theory?" defined the curriculum as "complicated conversations" with practitioners rather than the formulation of objectives evaluated by standardized tests. He posits that the curriculum is largely intertwined with the perceptions of the practitioners that teach it. 

I will record the audio from a focus group conversation between my trainees after 3 months of using the new subject knowledge audit. This open forum will hopefully reveal deeper insights than my questionnaires as this will involve discourse between eh trainee teachers. They will be asked to discuss the same questions given to mentors.

I will attempt to perform an Interpretative phenomenological analysis (IPA) to offer insight into to offer insights into how trainee teachers computing teachers makes sense of a subject knowledge development. This is in the domain of Hermeneutics as the theory and practice of interpretation where the interpretation can be justified.

The small sample size of trainees makes this approach possible.

I will use the same codification process as used with the questionnaires to interpret the focus group discussion.


#### Ethical considerations of research design

This study will alter the way trainees evaluate their subject knowledge. This self identification could impact their personal identity. Given the new audit refers to more practical skills, it is possible that confidence of my trainees could be affected. To mitigate this I will:
* Ensure the audit clearly describes that it "is not used for formal assessment"
* Ensure I collect feedback about the trainees level of confidence - both lichert scale and free-from feedback to detect any issues
* Continually involve trainees in discussions about subject knowledge development to support them

No changes need to be made to the course in general and no change to assessment or re-validation will be required.

All participants have completed consent forms and are aware that their data is collected anonymously so they will not be identifiable, but their responses will be publicly visible.

Two questionnaires will be given 3 months aspart. To match the trainees responses over the time period, rather than collecting a name I a collect a 'code-name' they can enter in each questionnaire to identify participants.


Results
-------

I have totalled the codified sentiments and discussed them below.
See Appendix for full results codification table. 

I was unable to identify progress from september to december. The questionnaire were anonymous and students forgot the fake-names/identifies that had used in the first ones.


### Results - Codification - Commentary

I will work though the major points from the codified responses giving commentary.


#### Scope2 - Topics out of alignment with teaching - 18 references

Universally trainees and mentors voiced that the new subject knowledge audit did not align with the schools curriculum.

> a lot of areas that have not changed as they have not been relevant to what I have been teaching.

The audit was meant to convey the whole 'world map' of the skills a computing teacher.
At this point trainees had been in school a total of 32 days (8 weeks * 4 days).
The misalignment comes from; My intention was to provide a tool for a whole teachers career. Trainees wanted a tool for the 1 year PGCE.
> ... there are several points that would not be covered in a Secondary School

> it adds an unnecessary overhead of a student teacher going to learn a specific point of this tracker which whilst may be useful knowledge will not be applied to the classroom setting

> the new subject knowledge audit does not meet some of the topics I have made lesson ... Basic construct in programming (Sequence, Selection and Iteration)

These lower level skills were not listed because to be able to write simple algorithms (like a bubble sort) you need all of these fundamentals. They are inferred by the higher level skills. This overlaps with Misunderstanding3 as this is a teacher skill audit and not a student skill audit. This would metaphorically be similar to listing basic skills in other domains such as explicitly listing addition/multiplication for maths.

> It should feel like it all fits together to tick off your GCSE lesson. 

Trainees did not want/need an overview of the discipline, they wanted a tick list for the year. This highlights that my goals are out of alignment with my learners goals.


#### Quantifiable1 - New audit quantifiable/measurable/specific - 11 references

Trainees felt the new levelling system allowed them to clearly identify their current level of knowledge for each item. However this is largely overshadowed to be irreverent as stated above with S2, these skills were out of alignment with the skills they needed.

> I still prefer the new audit model for its clarity and easy to answer the confidence for each topic


#### Misunderstandings(1-7) - 15 references

Trainees misunderstood the audit was a "for teachers knowledge" and not "tasks for students". This created tension between the trainees, mentors and damaged rapport with the mentors.

> I do worry that learning some of these topics will require me to spend time away from lesson planning for the students with who I am working in order to fit these topics into my classes

> difficulties of showing students completed work ... for many of these categories.

> The department management outright refused to allow me to teach according to this audit

Another misunderstanding was trainees believed they had to complete the whole audit.

> I would be unable to complete everything on the audit

> I haven't had time for significant work on my subject knowledge which is outside areas of the curriculum


#### Scope6 - Request more macro scope - 4 references

Some trainees requested broader scope in the items citing "Societal Impact" and "Privacy". My concern with this broad definitions is how would a trainee be able to measure their knowledge in these areas? They are so ephemeral I don't think they would provide value in an audit.

Trainees wanted more autonomy over their skill development.

> (old audit) gives me more flexibility to work on developing my skills in ways that I see fit, as opposed to a pre-prescribed way



### Likert scale questions

### Now you have used the new subject audit model. How do you feel about the previous and new models?
| Response | Count |
|-|-|
| I prefer the new audit model | 1 |
| I somewhat prefer the new audit model | 2 |
| I somewhat prefer the previous audit model | 2 |
| I prefer the previous audit model | 1 |

Conclusion: Ambivalent between old and new - trainees largely ignored the audit


### How do you currently feel about your current skill-set as described by the subject knowledge audit gap analysis?
| Response | Count |
|-|-|
I am slightly under-confident about my abilities | 3 |
I have no response | 1 |
I feel slightly confident about my abilities | 2 |

Conclusion: Ambivalent - trainees largely ignored the audit



Conclusions
-----------

The old subject knowledge audit provided little meaningful value because the categories were not well defined enough for a novice practitioner to identify areas of development

The new subject knowledge audit provided little meaningful value because the categories were not aligned with the institutions novice practitioners were teaching in.

My original concept, was that if teachers were able to practically demonstrate the theoretical computing concepts with practical application. This would improve the quality of lessons computing teachers were able to create. The average trainee does not have the practical skills to do this and there is not sufficient time within the course to develop these skills. In modifying to the subject knowledge audit to include a practical dimension I alienated mentors and demoralised trainees.

I wanted to show a 'complete world map' of the dimensions of the discipline of computing. Even with verbal guidance and supporting text, many of my trainees were unable to distinguish what 'could be possible over a lifetime of teaching computing' with 'what is required in the next few months to gain qualified teacher status'. They were unable to conceive the audit tool as something they might be referring to for the next 5 years+. This created confusion and stress for trainees.

As the audits were not assessment requirements, they were largely ignored by trainees and mentors.


### Reflecting on effectiveness of intervention

* Objectives (actions and measurable outcomes)
    * Ensure that levels of subject knowledge are unambiguous and evidencable
        * Somewhat met: Trainees agreed the new levels reduced ambiguity
    * Trainee teachers and school mentors to accurately identify their current working level and areas of need
        * Not met: Scope of new audit was outside of GCSE requirements
    * Provide a quantitative measure of subject knowledge progress over a 3-month period
        * Not met: Trainee teachers did not have time to focus on any activity not related directly to classroom teaching. Little to no progress on audit skills
    * Trainees should be able to self direct their subject knowledge development
        * Not relevant: Trainees largely ignored the new audit and only focused on immediate lesson planning

### Reflecting on key measures of success

1. There is majority preference for the new audit model
    * Not met: students did not use the old or new audits and were largely ambivalent to both
2. Changes to student confidence levels should remain minimal
    * Met, but not related to intervention: as the new audit was not assessable, most trainees ignored it
3. Students feel they can accurately justify their current level of subject knowledge from the audit
    * Not met: trainees largely ignored the audit
4. There is evidence to support that practical computing skills development has taken place
    * Not met: trainees were constrained (especially at this stage in the course), to teach the lessons provided by their placement school. Skill development was evidenced by the lessons that they were guided to teach.

### Reflecting on research questions

* Research Question 1: "Can trainee teachers accurately self-evaluate their subject knowledge"
    * "yes" given clear enough criteria.
* Research Question 2: "and use this to guide their subject knowledge development?"
    * The reality is "no"
    * The driver for trainee teacher subject knowledge development is largely coupled to the trainees placement school curriculum. Anything beyond this is pragmatically optional. Trainees entire focus is delivering the content required by their placement school and learning the fundamentals of how to teach. Additional subject knowledge development is not assessed and therefore not actioned.

#### Reflecting on the Scope of the school computing curriculum

Trainees cited many times "not on the computing curriculum". In a subject session I asked them "was AI on the curriculum". The were unable to cite where it was. Their answer was "AI was not on the curriculum".
The national curriculum is deliberately open for interpretation. Each school has the freedom form their own approach. 
Professor Simeon Peyton Jones cited in his NCCE address "Where is the AI". He states "we don't need to rewrite the national curriculum to include AI. AI is already inferred by the opening paragraph" (Peyton Jones 2021)
* > A high-quality computing education equips pupils to use computational thinking and creativity to understand and change the world
* > ... how digital systems work and how to put this knowledge to use through programming
* > ... at a level suitable for the future workplace and as active participants in a digital world
* > ... and have repeated practical experience of writing computer programs in order to solve such problems
AI is part of our world and we should be teaching it to our young people.
There are many freely available web accessible systems for teaching AI to whole class's of KS2, KS3 and KS4 students.

The 7 trainees in this survey had only 32 days of school experience. They have limited experience to make judgements about curriculum content.

KS3 is flexible and at the schools discretion. KS4 is examined by a national paper exam. To raise results, schools ensure KS3 has a progression pathway of the concepts in KS4. Thus leading to the prevailing opinion that many topics are "not in the curriculum" if there is not a direct exam question for them.


#### Next Study: Suggestions

Biggs and Tang 2011 describe the concept of "Constructive Alignment" where the learning objectives, assessment and lesson content are all in alignment. Biggs and Tang suggest that any change to one of these must involve changes to all three components. By changing only one aspect of my course it was clear that it was going to have limited impact and create confusion. Future studies that involve alterations to course materials should be carefully consider other aspects that need to be updated.

For a more academically rigorous study, it would be wise to ensure that someone external to the project provides the codification of questionnaire responses or interviews. This will counter some the inherent bias that the creators of the study may have. Interviews should also be conducted by 3rd parties for the same reason.

To get a complete picture of teacher skills development, the scope should be widened to study teachers with a range of years of service. This could reveal deeper insights into how teachers skills develop to inform meaningful changes to initial teacher education.


#### Actions to improve practice

My subject knowledge audit enhancements focused around developing practical computing skills. The reality is that students (and therefore teachers) don't need these practical skills. The requirement for teachers is that their students have the knowledge to pass the GCSE exam.

There is no need for a separate specialised teacher subject knowledge audit.
An exam board GCSE specification lists the knowledge/skills needed and has mechanisms for assessment/measurement.
Trainees teachers do not have the capacity to focus on developing sills for topics that their students are not directly assessed on.

Is up to the individual teachers or individual school computing departments to develop their own custom computing opportunities (if they have the capacity). These additional opportunities cannot (and probably 'should not') be controlled by external entities.

The government has provided £80 million funding for the NCCE upskill computer science teachers. They are offering nationally accredited courses and bursary's of £2500 for trainees to take to their first employing school. This gives trainees incentive and accreditation for their subject knowledge development. This funding is due to expire in the summer of 2022 (Sentance 2019). It will be interesting to see what incentives are offered to computing trainees in future.

Actions for my teaching next academic year 2022/2023:
* Replace the 'trainee teacher subject knowledge audit' with an amalgamation of current GCSE specifications from active exam boards
    * NCCE (“Computer Science Accelerator Programme”) provides a mapping tool for exam board specification to available courses
* Ask student teachers to judge their level of skill by:
    * Self administering and marking their own attempt at GCSE past papers
    * Using existing NCCE self assessment tooling. (“Diagnostic and Summative Assessment of Computing”) online tool
* Remove any reference to A-Level topics - these are beyond the scope of the course
* Direct trainee teachers towards existing skills certifications NCCE (“Computer Science Accelerator Programme”)
    * By doing this trainee teachers accrue bursaries to take to their first employing school. This manifests externally visible and verifiable progress
* Give trainee teachers the opportunity to sit a real GCSE computing exam in a school and be graded
    * (In reality this should be a mandatory requirement of any trainee in any subject)


#### What next?

Industry is taking measures into their own hands. Many employers are loosing faith in the weight behind formal computing qualifications. Industry is inventing their own professional certification and their own recruitment screening process that is largely based on practical skills. (Denning, more citation needed, this is common)

A follow-up questions might be: "What is the purpose of Computing Education in secondary schools", "What should schools be focusing on?" and "How can we make meaningful positive change to computing education?".

Speaking to undergraduates and local school teachers, there is a growing desire to develop practical computing skills that formal education is not catering for. Coder Dojo's (coderdojo.com), Coding Dojo (codingdojo.org), Code Club's (codeclub.org) and other independent groups are setting up all over the world to cater for the missing parts of computing from mainstream education. I am in the process of investigating how to provide weekly group that spans secondary students, though undergrads, though junior developers. As an institution, Canterbury Christ Church is in a great position to host such a group. It would be interesting to see if this approach is viable and will be the focus of my next investigation.




---

?? In a conversation with Tig Williams (Teacher Educator at Southampton University and Lead for the NCCE Accelerator program) in December 2021, 

Cathrin Sherwood - Librarian access to journals

"What" is your intervention

Graduate attributes - know what they do/donot know (higher educaiton oeprates)



Missing from the audit - word skills (ment to be computing)
Complete the whole audit - IT SAYS THIS AT THE TOP - student teachers do not read

They can't all buy a domain name - THIS IS NOT FOR STUDENTS!! IT'S FOR TEACHERS/YOU

Tig

Huffman AQA only - never seen an exam question on this



* Conclusions are clearly related to objectives of the assignment and the work shows some originality.
* You justify these in relation to past or future challenges in your practice. 
* explicitly considers the limitations of the evidence.



Mimir - version control - convention cloud doucments are not sufficent for code (multiple files) - changes over time

    * You discuss the outcomes from your evaluation and their implications for your practice across multiple domains (e.g., departmental, disciplinary, institutional and/or sector-wide implications).
    * Throughout the work you analyse relevant, detailed and specific examples from your practice in the light of educational research and theory. 
    * This reflection on your practice provides a clear vision and basis of evidence for improving your practice. 


* What have you learned from doing this study?
* How can the information be useful to others?

Notes
=====

Dont ask "why", but ask who, when (timefram), how (mehtod)
Succes/ not scucess

Aims /= methodology

Aims = raise points
Objective identify steps
Objectives == how to get to aim

Who made "comments" D2 descriptors

QA (know to do these things)
Module specifications
External examiners
Learning and teaching strategy

"What" is your intervention

Research focus need to be lifted up

Graduate attributes - know what they do/donot know (higher educaiton oeprates)


Wont destroy until grades
not shared on public platform


---


V1 - diverse
    ICT or Computing background
V2 - partcipation - differnt formats
V4 - wider context - world outside of

K5 - how do you know your teaching is effective



    * D2 - Fellow
        1. Successful incorporation of subject and pedagogic research and/or scholarship within the above activities, as part of an integrated approach to academic practice
        2. Successful engagement in continuing professional development in relation to teaching, learning, assessment and, where appropriate, related professional practices


    * Areas of Activity
        * A1
            * Design and plan learning activities and/or programmes of study
        * A2
            * Teach and/or support learning
        * A3
            * Assess and give feedback to learners
        * A4
            * Develop effective learning environments and approaches to student support and guidance
        * A5
            * Engage in continuing professional development in subjects/disciplines and their pedagogy, incorporating research, scholarship and the evaluation of professional practice
    * Core Knowledge
        * K1
            * The subject material
        * K2
            * Appropriate methods for teaching, learning and assessing in the subject area and at the level of the academic programme
        * K3
            * How students learn, both generally and within their subject/disciplinary area(s)
        * K4
            * The use and value of appropriate learning technologies
        * K5
            * Methods for evaluating the effectiveness of teaching
        * K6
            * The implications of quality assurance and quality enhancement for academic and professional practice with a particular focus on teaching
    * Professional Values
        * V1
            * Respect individual learners and diverse learning communities 
        * V2
            * Promote participation in higher education and equality of opportunity for learners
        * V3
            * Use evidence-informed approaches and the outcomes from research, scholarship and continuing professional development 
        * V4
            * Acknowledge the wider context in which higher education operates  recognising the implications for professional practice





Mark scheme
* Focus of assignment
    * Critical reflection on the D2 descriptor in relation to your own practice.

    
* Evaluation of practice
    * In the work you evaluate your practice using evidence from a range of sources and perspectives (e.g. self-reflection; student feedback; student assessment performance; observation of teaching by another educator; external examination reports). 

    

* Reflection and application to your own practice
* Engagement with scholarship and research
    * The work engages with a wide range of research and scholarship, including disciplinary pedagogies or scholarly debates within educational research. 
    * The sources cited are critically analysed and evaluated in the light of your own practice and issues implicit in the literature are made explicit and integral to the argument.
    * The work draws on scholarship from your own and other disciplines
* QWC
    * Aims
    * Logically ordered
    * Harvard
    * Figures labelled
    * proof read

Analyse the method used


The positivist paradigm is based in the assumption that a single tangible reality exists—one that can be understood, identified, and measured.

Case Study: Identify the problems.
Select the major problems in the case.
Suggest solutions to these major problems.
Recommend the best solution to be implemented.
Detail how this solution should be implemented.



* [Editorial - Developing Computationally Literate Teachers: Current Perspectives and Future Directions for Teacher Preparation in Computing Education](https://www.learntechlib.org/p/184602/)
    * Cant access



"Magic Rocks"

Interview candidate "I'm a practical person. I learn things when I need to. If I need a skill, I can learn it the night before"
"In a GCSE classroom with 30 students, you need to be able to glance at code and diagnose/debug it in around 6 to 10 seconds. This is NOT the night before"

Maybe my bar is too high
Maybe most of the things a 13 year old can tackel can be learnt by an adult 24 hours beforehand

If my mentors don't engage with this. It is meaningless to assert my opinion if I alienate the profetionals I have to work with



(Denning)
* > use competency-based skill assessments to measure student progress. Be wary of the claim of universal value, for it has little empirical support

(Nijenhuis-Voogt et al.)
* Conflicting approaches
        * "Thinking" (just the concept) or "Thinking and Making" (doing it in code)
        * > Transcribing to real code is not carried out by my students. [After writing the algorithm in pseudo-code], it does not add any value … at that time, they are at the level of understanding where they should be
        * > I want my students to be able to specify a logical solution for a problem on the level of elementary building blocks so you can make a one-on-one translation to a programming language
        * > Students will make something so they really get an idea what it involves rather than remaining theoretical


* > "What we miss in secondary education is to first think about what is needed, what is the goal, how we get there, and what steps can be distinguished."
    * > "We assess the implementation too much and I think we assess conceptual knowledge insufficiently. How do you assess whether students really understand a concept?"




[HCDA: From Computational Thinking to a Generalized Thinking Paradigm: As a new era in computing emerges, so too must our fundamental thinking patterns.](https://dl.acm.org/doi/10.1145/3418291) 2021
Argues that computation thinking alone is not enough to solve modern day problems and propose additions, we need to pair this with historical, data and archectural thinging

[Still a New Kid on the Block? Computational Thinking as Problem Solving in Code.org.](https://journals.sagepub.com/doi/10.1177/0735633120972050)
TODO


> As long as we keep designing and trying out our innovative pedagogies, we maintain the discourse on the need to change, deepen and extend our collective knowledge as to how (and how not to) improve the current situation, pushing the system to pursue a new state of equilibrium
(Kolikant)
